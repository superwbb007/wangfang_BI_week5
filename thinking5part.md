**Thinking1	在实际工作中，FM和MF哪个应用的更多，为什么？FM应用较多。**FM矩阵将User和Item都进行了one-hot编码作为特征，使得特征维度非常巨大且稀疏；**矩阵分解MF是FM的特例，即特征只有User ID和Item ID的FM模型；**矩阵分解MF只适用于评分预测，进行简单的特征计算，无法利用其他特征；FM引入了更多的辅助信息作为特征；FM在计算二阶特征组合系数的时候，使用了MF。FM算法的应用场景：回归问题（损失函数课采用least square error）和二分类问题(损失函数可采用(logistic loss)。FM只适合二阶的。可以用于CTR预估,FM一般用于排序，数据量不大时，可同时应用于召回和排序。		
Thinking2	**FFM与FM有哪些区别？FM算法：每个特征只有一个隐向量，FM是FFM的特例。FFM算法：计算粒度更细，计算更慢。每个特征有多个隐向量，使用哪个，取决于和哪个向量进行点乘，通过引入field的概念，FFM把相同性质的特征归于同一个field。隐向量的长度为 k，FFM的二次参数有 nfk 个，多于FM模型的 nk 个.由于隐向量与field相关，FFM二次项并不能够化简，计算复杂度是O(k*n^2)FFM的k值一般远小于FM的k值
Thinking3	DeepFM相比于FM解决了哪些问题，原理是怎样的?FM可以做特征组合，但是计算量大，一般只考虑2阶特征组合,DeepFM既考虑低阶（1阶+2阶），又能考虑到高阶特征 => DeepFM=FM+DNN,设计了一种end-to-end的模型结构 => 无须特征工程,共享特征输入.在各种benchmark和工程中效果好.Deep模型:原始特征向量维度非常大，高度稀疏，为了更好的发挥DNN模型学习高阶特征的能力，设计子网络结构（从输入层=>嵌入层），将原始的稀疏表示特征映射为稠密的特征向量。Input Layer => Embedding Layer,不同field特征长度不同，但是子网络输出的向量具有相同维度k;利用FM模型的隐特征向量V作为网络权重初始化来获得子网络输出向量.在推荐系统中，对于离线变量我们需要转换成one-hot => 维度非常高，可以将其转换为embedding向量,原来每个Field i维度很高，都统一降成k维embedding向量.方法：接入全连接层，对于每个Field只有一个位置为1，其余为0，因此得到的embedding. FM模型和Deep模型中的子网络权重共享，也就是对于同一个特征，向量Vi是相同的. DeepFM中的模块：Sparse Features，输入多个稀疏特征；Dense Embeddings：对每个稀疏特征做embedding，学习到他们的embedding向量(维度相等，均为k），因为需要将这些embedding向量送到FM层做内积。同时embedding进行了降维，更好发挥Deep Layer的高阶特征学习能力。FM Layer：一阶特征：原始特征相加，二阶特征：原始特征embedding后的embedding向量两两内积；Deep Layer，将每个embedding向量做级联，然后做多层的全连接，学习更深的特征；Output Units，将FM层输出与Deep层输出进行级联，接一个dense层，作为最终输出结果。Thinking4	Surprise工具中的baseline算法原理是怎样的？BaselineOnly和KNNBaseline有什么区别？	Baseline算法：基于统计的基准预测线打分。bui=u+bu+bi,bui 预测值,bu 用户对整体的偏差,bi 商品对整体的偏差.BaselineOnly和KNNBaseline都是应用ALS交替最小二乘法，区别是前者是直接进行计算，后者的整个大框架是基于邻域完成的
Thinking5	基于邻域的协同过滤都有哪些算法，请简述原理.Usercf和Itemcf.	Usercf利用行为的相似度计算用户的相似度，给用户推荐和他兴趣相似的其他用户喜欢的物品。Itemcf利用行为的相似度计算物品的相似度，给用户推荐和他之前喜欢的物品相似的物品。Itemcf推荐的可解释性更强，用户数远大于物品数时，使用itemcf;用户数少于物品数时，使用UserCF更准确。实时性上，UserCF对用户新的行为不一定导致推荐结果的变化，Itemcf对用户新的行为会导致推荐结果的变化。在冷启动阶段，UserCF对于新加入的物品能很快进入推荐列表，Itemcf对新加入的用户可以很快进行推荐。
